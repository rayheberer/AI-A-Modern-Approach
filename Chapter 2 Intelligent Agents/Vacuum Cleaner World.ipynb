{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9\n",
    "\n",
    "__Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.2 and specified on page 38. Your implementation should be modular so that the sensors, actuators, and environment characteristics (size, shape, dirt placement, etc.) can be changed easily. (_Note:_ for some choices of programming language and operating system there are already implementations in the [online code repository](http://aima.cs.berkeley.edu/code.html).)__\n",
    "\n",
    "The world in Figure 2.2 has two squares, \"A\" and \"B\". I have implemented this as `vacuum_cleaner_world.environments.SimpleVacuumWorld`.\n",
    "\n",
    "The specifications from page 38 are as follow:\n",
    "\n",
    "* The performance measure awards one point for each clean square at each time step, over a  \"lifetime\" of 1000 time steps.\n",
    "* The geography of the environment is known _a priori_ but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The _Left_ and _Right_ actions move the agent left and right except when this would take the agent outside the environment, in which case the agent remains where it is.\n",
    "* The only available actions are _Left_,  _Right_, and _Suck_.\n",
    "* The agent correctly perceives its location and whether that location contains dirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.environment import SimpleVacuumWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10\n",
    "__Consider a modified version of the vacuum environment in Exercise 2.9, in which the agent is penalized one point for each movement.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Can a simple reflex agent be perfectly rational for this environment? Explain.__\n",
    "\n",
    "A simple reflex agent cannot be perfectly rational. Since clearly a reflex agent that returns the action `Clean` when the dirt sensor informs it that there is dirt in its location will do better than one that moves, the action for percepts `[A, Dirt]` and `[B, Dirt]` will be `Clean`. For `[A, No Dirt]` a simple reflex agent can either return `Clean`, `Left`, or `Right`.\n",
    "\n",
    "`Clean` or `Left` will cause the agent to stay in the same place for its lifetime. The expected performance of this agent will be poor, if square B is expected to have dirt. However, a reflex agent which returns `Right` for `[A, No Dirt]` and `Left` for `[B, No Dirt]` will oscillate back and forth once both squares are cleaned. An agent that stays still after visiting both squares would perform better, but no set of condition-action rules,  and therefore no simple reflex agent, can implement such an agent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import SimpleReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(trials, env, AgentObject, agent_name, **kwargs):\n",
    "    scores = [env.simulate(AgentObject, **kwargs) for _ in range(trials)]\n",
    "    \n",
    "    print('{} had an average performance of {:.2f} over {} trials'.format(agent_name,\n",
    "                                                                          sum(scores)/len(scores), \n",
    "                                                                          trials))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Reflex Agent had an average performance of 1000.29 over 100 trials\n"
     ]
    }
   ],
   "source": [
    "env = SimpleVacuumWorld(move_penalty=True)\n",
    "\n",
    "run_trials(100, env, SimpleReflexAgent, 'Simple Reflex Agent');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be sufficient to show that other agents can do better than this to prove that `SimpleReflexAgent` is not rational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. What about a reflex agent with state? Design such an agent.__\n",
    "\n",
    "Since it takes just 1 movement to visit every square, a reflex agent in this environment only needs to keep track of how many moves it has made, and return `NoOp` if this is equal to 1.\n",
    "\n",
    "The stateful reflex agent is implemented as `vacuum_cleaner_world.agents.StatefulReflexAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum scores attainable are:\n",
    "\n",
    "__1999__ if there is initially no dirt, or dirt in the agent's starting square...\n",
    "\n",
    "__1998__ if there is dirt in the opposite square...\n",
    "\n",
    "__1997__ if there is initially dirt in both squares...\n",
    "\n",
    "Assuming that the first score taken is after the agent takes its first action.\n",
    "\n",
    "By manually setting the initial dirt and agent location, I show that the `StatefulReflexAgent` is rational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import StatefulReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_all_dirt_agent_configs(AgentObject, **kwargs):\n",
    "    dirt_inits = ['dirty', [0, 1], [1, 0], [0, 1], [1, 0], 'clean']\n",
    "    agent_inits = [None, 'A', 'B', 'B', 'A', None]\n",
    "    simulation_names = ['Both Squares Dirty', \n",
    "                        'Agent in A, Dirt in B',\n",
    "                        'Agent in B, Dirt in A',\n",
    "                        'Agent in B, Dirt in B',\n",
    "                        'Agent in A, Dirt in A',\n",
    "                        'Both Squares Clean']\n",
    "    \n",
    "    scores = []\n",
    "    for dirt_init, agent_init, name in zip(dirt_inits, agent_inits, simulation_names):\n",
    "        env = SimpleVacuumWorld(dirt_init=dirt_init, init_loc=agent_init, **kwargs)\n",
    "        score = env.simulate(AgentObject)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return simulation_names, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Squares Dirty: 1997\n",
      "Agent in A, Dirt in B: 1998\n",
      "Agent in B, Dirt in A: 1998\n",
      "Agent in B, Dirt in B: 1999\n",
      "Agent in A, Dirt in A: 1999\n",
      "Both Squares Clean: 1999\n"
     ]
    }
   ],
   "source": [
    "names, scores = simulate_all_dirt_agent_configs(StatefulReflexAgent, move_penalty=True)\n",
    "for name, score in zip(names, scores):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c. How do your answers to *a* and *b* change if the agent's percepts give it the clean/dirty status of every square in the environment?__\n",
    "\n",
    "A simple reflex agent can be perfectly rational in this environment. Therefore, a reflex agent with state can also be perfectly rational, since it can have the same set of condition-action rules as the simple reflex agent, and maintain a state variable that does not do anything (e.g. always being equal to 0).\n",
    "\n",
    "I implement a simple reflex agent with access to all percepts as `vacuum_cleaner_world.agents.FullInfoReflexAgent`. Its condition action rules are:\n",
    "\n",
    "| Percepts                    | Action | \n",
    "| --------------------------- |:------:|\n",
    "| (`A`, `Dirt A`, `Dirt B`)   | `Suck` | \n",
    "| (`B`, `Dirt A`, `Dirt B`)   | `Suck` | \n",
    "| (`B`, `Clean A`, `Dirt B`)  | `Suck` | \n",
    "| (`A`, `Dirt A`, `Clean B`)  | `Suck` | \n",
    "| (`A`, `Clean A`, `Dirt B`)  | `Right`| \n",
    "| (`B`, `Dirt A`, `Clean B`)  | `Left` | \n",
    "| (`A`, `Clean A`, `Clean B`) | `NoOp` | \n",
    "| (`B`, `Clean A`, `Clean B`) | `NoOp` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import FullInfoReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Squares Dirty: 1997\n",
      "Agent in A, Dirt in B: 1998\n",
      "Agent in B, Dirt in A: 1998\n",
      "Agent in B, Dirt in B: 2000\n",
      "Agent in A, Dirt in A: 2000\n",
      "Both Squares Clean: 2000\n"
     ]
    }
   ],
   "source": [
    "names, scores = simulate_all_dirt_agent_configs(FullInfoReflexAgent, \n",
    "                                                move_penalty=True, \n",
    "                                                perfect_information=True)\n",
    "\n",
    "for name, score in zip(names, scores):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this agent receives more percepts, its standard of rationality is higher. It can achieve a perfect score of 2000 since it can know with certainty that there is no need to visit the other square if there is no dirt there initially. Note that the reflex agent with state from __b__ is still rational, because given its percepts, its expected performance measure is low if it does not visit each square once since there is some probability of it finding dirt in the square it did not start in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 \n",
    "\n",
    "__Consider a modified version of the vacuum environment in Exercise 2.9, in which the geography of the environment - its extent, boundaries, and obstacles - is unknown, as is the initial dirt configuration. (The agent can go *Up* and *Down* as well as *Left* and *Right*.)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Can a simple reflex agent be perfectly rational for this environment? Explain.__\n",
    "\n",
    "A simple reflex agent cannot be rational, because it cannot maintain a model of the environment, which would be necessary for it to know and remember where squares are situated in the initially unknown geography. Even if it were to perceive that it was in square `A`, this information would not mean anything, because the agent would not know which squares, if any, would be next to square `A`. Since the geography of the environment is unknown, the only percept information a reflex agent can act on is that of whether there is dirt or no dirt in its current square. Therefore, the only implementable agent functions are those where the agent cleans, and then moves in a single direction whenever there is no dirt in its current square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.environment import UnknownVacuumWorld\n",
    "from vacuum_cleaner_world.agents import SimpleReflexAgentUnknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geography = [['AA', 'AB', 'AC', None],\n",
    "             ['BA', 'BB', 'BC', 'BD'],\n",
    "             ['CA', None, 'CC', 'CD'],\n",
    "             ['DA', 'DB', 'DC', 'DD']]\n",
    "\n",
    "env = UnknownVacuumWorld(geography=geography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=|   \n",
      "|=||=||=||=|\n",
      "|=|   |=||=|\n",
      "|=||=||=||=|\n"
     ]
    }
   ],
   "source": [
    "env.display_geography()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Reflex Agent (Traveling Up) had an average performance of 7908.76 over 100 trials\n",
      "First 10 scores: [8000, 5998, 9000, 7999, 9000, 9996, 6998, 8997, 10000, 7000] \n",
      "\n",
      "Simple Reflex Agent (Traveling Down) had an average performance of 8198.41 over 100 trials\n",
      "First 10 scores: [12000, 9000, 6000, 6000, 6000, 6000, 8000, 10998, 8992, 7999] \n",
      "\n",
      "Simple Reflex Agent (Traveling Left) had an average performance of 7979.12 over 100 trials\n",
      "First 10 scores: [10000, 8997, 10000, 6000, 6998, 8000, 10999, 8000, 8000, 11000] \n",
      "\n",
      "Simple Reflex Agent (Traveling Right) had an average performance of 7788.71 over 100 trials\n",
      "First 10 scores: [9000, 7000, 6000, 9000, 6000, 5997, 7000, 11000, 8000, 7000] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [' (Traveling Up)', ' (Traveling Down)', ' (Traveling Left)', ' (Traveling Right)']\n",
    "directions = ['Up', 'Down', 'Left', 'Right']\n",
    "\n",
    "for name, direction in zip(names, directions):\n",
    "    scores = run_trials(100, env, SimpleReflexAgentUnknown, \n",
    "                        \"Simple Reflex Agent\"+name, direction=direction)\n",
    "    print('First 10 scores:', scores[:10], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. Can a simple reflex agent with a _randomized_ agent function outperform a simple reflex agent? Design such an agent and measure its performance on several environments.__\n",
    "\n",
    "An agent that moves randomly when its current square is clean will outperform an agent that moves in a single direction in most environments, excepting those with the geography of a straight line where the simple reflex agent happens to start at one end and has condition action rules that cause it to move towards the other end after cleaning. Outside of these special cases, random movements will explore the environment more effectively, if not particularly efficiently overall.\n",
    "\n",
    "My implementation initializes each square with a 50% chance of containing dirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import RandomizedReflexAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample Environment 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=|   \n",
      "|=||=||=||=|\n",
      "|=|   |=||=|\n",
      "|=||=||=||=|\n"
     ]
    }
   ],
   "source": [
    "env.display_geography()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized Reflex Agent had an average performance of 13698.22 over 100 trials\n",
      "First 10 scores: [13858, 13792, 13869, 13811, 13884, 13673, 13634, 13838, 13734, 13816]\n"
     ]
    }
   ],
   "source": [
    "scores = run_trials(100, env, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample Environment 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=||=||=|\n",
      "|=|      |=||=|\n",
      "|=|      |=|   \n",
      "|=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 14462.54 over 100 trials\n",
      "First 10 scores: [14531, 14729, 14661, 14781, 14842, 14586, 14192, 14744, 14845, 14688] \n",
      "\n",
      "Simple Reflex Agent (Traveling Left) had an average performance of 8617.63 over 100 trials\n",
      "First 10 scores: [5997, 8992, 6000, 10000, 4998, 7996, 8997, 8000, 7996, 8996]\n"
     ]
    }
   ],
   "source": [
    "geography2 = [['AA', 'AB', 'AC', 'AD', 'AE'],\n",
    "              ['BA', None, None, 'BD', 'BE'],\n",
    "              ['CA', None, None, 'CD', None],\n",
    "              ['DA', 'DB', 'DC', 'DD', 'DE']]\n",
    "\n",
    "env2 = UnknownVacuumWorld(geography=geography2)\n",
    "env2.display_geography()\n",
    "\n",
    "scores = run_trials(100, env2, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10], '\\n')\n",
    "\n",
    "scores = run_trials(100, env2, SimpleReflexAgentUnknown, \n",
    "                    \"Simple Reflex Agent (Traveling Left)\", direction='Left')\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample Environment 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=|      \n",
      "      |=|      \n",
      "      |=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 6869.57 over 100 trials\n",
      "First 10 scores: [6972, 6838, 6909, 6829, 6990, 6943, 6899, 6816, 6828, 6927] \n",
      "\n",
      "Simple Reflex Agent (Traveling Left) had an average performance of 4578.99 over 100 trials\n",
      "First 10 scores: [4000, 4000, 7000, 2999, 4000, 4000, 2998, 4000, 4000, 4000]\n"
     ]
    }
   ],
   "source": [
    "geography3 = [['AA', 'AB', 'AC', None, None],\n",
    "              [None, None, 'BC', None, None],\n",
    "              [None, None, 'CC', 'CD', 'CE']]\n",
    "\n",
    "env3 = UnknownVacuumWorld(geography=geography3)\n",
    "env3.display_geography()\n",
    "\n",
    "scores = run_trials(100, env3, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10], '\\n')\n",
    "\n",
    "scores = run_trials(100, env3, SimpleReflexAgentUnknown, \n",
    "                    \"Simple Reflex Agent (Traveling Left)\", direction='Left')\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample Environment 4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 6897.85 over 100 trials\n",
      "First 10 scores: [6927, 6776, 6972, 6786, 6888, 6979, 6737, 6930, 6765, 6903] \n",
      "\n",
      "Simple Reflex Agent (Traveling Left) had an average performance of 5543.25 over 100 trials\n",
      "First 10 scores: [3992, 5995, 4993, 4997, 3000, 6984, 5996, 4000, 3000, 5992]\n"
     ]
    }
   ],
   "source": [
    "geography4 = [['A', 'B', 'C', 'D', 'E', 'F', 'G']]\n",
    "\n",
    "env4 = UnknownVacuumWorld(geography=geography4)\n",
    "env4.display_geography()\n",
    "\n",
    "scores = run_trials(100, env4, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10], '\\n')\n",
    "\n",
    "scores = run_trials(100, env4, SimpleReflexAgentUnknown, \n",
    "                    \"Simple Reflex Agent (Traveling Left)\", direction='Left')\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample Environment 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |=||=||=|   \n",
      "|=||=|   |=||=|\n",
      "|=||=|   |=||=|\n",
      "   |=||=||=|   \n",
      "Randomized Reflex Agent had an average performance of 13654.55 over 100 trials\n",
      "First 10 scores: [13680, 13848, 13700, 13501, 13534, 13713, 13624, 13845, 13839, 13811] \n",
      "\n",
      "Simple Reflex Agent (Traveling Up) had an average performance of 8158.95 over 100 trials\n",
      "First 10 scores: [7000, 6000, 9000, 6999, 9000, 7000, 10000, 8998, 10994, 10000]\n"
     ]
    }
   ],
   "source": [
    "geography5 = [[None, 'AB', 'AC', 'AD', None],\n",
    "              ['BA', 'BB', None, 'BD', 'BE'],\n",
    "              ['CA', 'CB', None, 'CD', 'CE'],\n",
    "              [None, 'DB', 'DC', 'DD', None]]\n",
    "\n",
    "env5 = UnknownVacuumWorld(geography=geography5)\n",
    "env5.display_geography()\n",
    "\n",
    "scores = run_trials(100, env5, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10], '\\n')\n",
    "\n",
    "scores = run_trials(100, env5, SimpleReflexAgentUnknown, \n",
    "                    \"Simple Reflex Agent (Traveling Up)\", direction='Up')\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c. Can you design an environment in which your randomized agent will perform poorly? Show your results.__\n",
    "\n",
    "If the geography of the environment contains bottlenecks,then a randomized agent may have a tough time navigating through them by chance and reaching all the dirty squares. To demonstrate that a bottleneck is responsible for lowered performance, I will also show agent performance on an environment with the same total number of squares, but no bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "      |=|      \n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 18866.78 over 100 trials\n",
      "First 10 scores: [19836, 18455, 19642, 19472, 18982, 17280, 18998, 18292, 19995, 15532]\n"
     ]
    }
   ],
   "source": [
    "bottle_geo = [['AA', 'AB', 'AC', 'AD', 'AE'],\n",
    "              ['BA', 'BB', 'BC', 'BD', 'BE'],\n",
    "              [None, None, 'CC', None, None],\n",
    "              ['DA', 'DB', 'DC', 'DD', 'DE'],\n",
    "              ['EA', 'EB', 'EC', 'ED', 'EE']]\n",
    "\n",
    "bottle_env = UnknownVacuumWorld(geography=bottle_geo, dirt_init='dirty')\n",
    "bottle_env.display_geography()\n",
    "\n",
    "scores = run_trials(100, bottle_env, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "      |=|      \n",
      "Randomized Reflex Agent had an average performance of 19814.11 over 100 trials\n",
      "First 10 scores: [20070, 19844, 19608, 19737, 19590, 19429, 19635, 19690, 20023, 19857]\n"
     ]
    }
   ],
   "source": [
    "non_bottle_geo = [['AA', 'AB', 'AC', 'AD', 'AE'],\n",
    "                  ['BA', 'BB', 'BC', 'BD', 'BE'],\n",
    "                  ['CA', 'CB', 'CC', 'CD', 'CE'],\n",
    "                  ['DA', 'DB', 'DC', 'DD', 'DE'],\n",
    "                  [None, None, 'EC', None, None]]\n",
    "\n",
    "non_bottle_env = UnknownVacuumWorld(geography=non_bottle_geo, dirt_init='dirty')\n",
    "non_bottle_env.display_geography()\n",
    "\n",
    "scores = run_trials(100, non_bottle_env, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "print('First 10 scores:', scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d. Can a reflex agent with state outperform a simple reflex agent? Design such an agent and measure its performance on several environments. Can you design a rational agent of this type?__\n",
    "\n",
    "A reflex agent with state can keep track of the squares it has visited, and therefore should be able to implement basic navigation strategies to explore the space, such as zigzagging.\n",
    "\n",
    "The problem of mapping out and keeping track of one's own position within an environment is known as Simultaneous Localization and Mapping (SLAM). [As far as I am aware](https://www.quora.com/Is-the-SLAM-simultaneous-localization-and-mapping-problem-in-robot-navigation-solved-for-dynamic-changing-environments), it is not considered a \"solved\" problem even for static environments.\n",
    "\n",
    "A rational agent would need to have a prior belief over the set of possible grid topologies (boundaries and obstacles) in order to be able to calculate its expected performance and choose the strategy for traversing the unknown space. Given some prior, designing a rational reflex agent with state may be possible, but would likely be intractable.\n",
    "\n",
    "I implement an agent that explores in a depth-first manner, going in one direction and adding to a queue of explored squares and the directions unexplored from there. When it hits an obstacle, it changes directions. When all directions away from a square are explored, it is taken off the list. When an agent has no directions to explore from its current square, it backtracks to the most recent square with adjacent directions left unexplored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import DepthStatefulReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 1\n",
      "|=||=||=|   \n",
      "|=||=||=||=|\n",
      "|=|   |=||=|\n",
      "|=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 13724.88 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 13873.74 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 2\n",
      "|=||=||=||=||=|\n",
      "|=|      |=||=|\n",
      "|=|      |=|   \n",
      "|=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 14454.55 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 14862.53 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 3\n",
      "|=||=||=|      \n",
      "      |=|      \n",
      "      |=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 6883.42 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 6966.82 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 4\n",
      "|=||=||=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 6874.36 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 6962.62 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 5\n",
      "   |=||=||=|   \n",
      "|=||=|   |=||=|\n",
      "|=||=|   |=||=|\n",
      "   |=||=||=|   \n",
      "Randomized Reflex Agent had an average performance of 13614.94 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 13879.64 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 6\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "      |=|      \n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 18751.62 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 20214.15 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 7\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "      |=|      \n",
      "Randomized Reflex Agent had an average performance of 19839.07 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 20286.10 over 100 trials\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "envs = [env, env2, env3, env4, env5, bottle_env, non_bottle_env]\n",
    "\n",
    "for i, envm in enumerate(envs, 1):\n",
    "    print('Environment', i)\n",
    "    envm.display_geography()\n",
    "    run_trials(100, envm, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "    run_trials(100, envm, DepthStatefulReflexAgent, \"Stateful Reflex Agent\")\n",
    "    print('------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reflex agent with state outperforms the randomized reflex agent on a variety of environments, and does so by a wide margin in the environment containing a bottleneck which was shown to be challenging for a randomized agent to navigate. This shows that the stateful reflex agent is taking advantage of the information it collects to better explore the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12\n",
    "\n",
    "__Repeat Exercise 2.11 for the case in which the location sensor is replaced with a \"bump\" sensor that detects the agent's attempts to move into an obstacle or to cross the boundaries of the environment. Suppose the bump sensor stops working; how should the agent behave?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Can a simple reflex agent be perfectly rational for this environment? Explain.__\n",
    "\n",
    "A simple reflex agent with bump sensors cannot be perfectly rational, for the same reasons as in __2.11__. It can have slightly more sophisticated behavior (going one direction if it encounters a bump, and another if it doesn't), but this is not enough to explore the environment and attain an expected performance measure that no agent receiving the same percepts could outperform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. Can a simple reflex agent with a _randomized_ agent function outperform a simple reflex agent? Design such an agent and measure its performance on several environments.__\n",
    "\n",
    "Again, randomization allows for more effective exploration of an environment with unknown geography, meaning a simple reflex agent with a randomized agent function will outperform its deterministic counterpart. Neither `SimpleReflexAgentUnknown` nor `RandomizedReflexAgent` even made use of the Location percept in their agent functions, so the simulation results from __2.11__ transfer over exactly (being able to specify different actions in response to bump/no-bump is negligible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c. Can you design an environment in which your randomized agent will perform poorly? Show your results.__\n",
    "\n",
    "As was the case for __b__, the results from __2.11__ hold for randomized simple reflex agents with bump sensors, since in environments with unknown geography, neither bump nor location information is helpful for an agent without state. To have behavior such as continuing in one direction when a square is clean and changing directions randomly on sensing a bump, an agent would need to store as its state the current direction it is traveling in. Without this capability, the implementation of `RandomizedReflexAgent` is a valid design for an agent that has a bump sensor, since it does not make use of this information. Thus, the bottleneck geographies demonstrated in __2.11__ will remain challenging for randomized agents with bump sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d. Can a reflex agent with state outperform a simple reflex agent? Design such an agent and measure its performance on several environments. Can you design a rational agent of this type?__\n",
    "\n",
    "A reflex agent with state can outperform a simple reflex agent in this setup for the same reasons as in __2.11__. Though it does not directly sense its location, it can infer that it has moved to a new square if it undertakes a movement action and does not sense a bump. Because of this, it can implement strategies like depth-first exploration by storing internal histories of movements and directions explored.\n",
    "\n",
    "Swapping out a location sensor for a bump sensor does not make the task of rationally exploring an unknown geography any more tractable than before. I cannot design a rational agent here any more than I could in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 1\n",
      "|=||=||=|   \n",
      "|=||=||=||=|\n",
      "|=|   |=||=|\n",
      "|=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 13699.55 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 8647.36 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 2\n",
      "|=||=||=||=||=|\n",
      "|=|      |=||=|\n",
      "|=|      |=|   \n",
      "|=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 14487.67 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 8678.15 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 3\n",
      "|=||=||=|      \n",
      "      |=|      \n",
      "      |=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 6885.30 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 4528.11 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 4\n",
      "|=||=||=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 6879.31 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 4468.46 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 5\n",
      "   |=||=||=|   \n",
      "|=||=|   |=||=|\n",
      "|=||=|   |=||=|\n",
      "   |=||=||=|   \n",
      "Randomized Reflex Agent had an average performance of 13645.28 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 8377.52 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 6\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "      |=|      \n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "Randomized Reflex Agent had an average performance of 18672.76 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 2355.15 over 100 trials\n",
      "------------------------------------------------------------------------------\n",
      "Environment 7\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "|=||=||=||=||=|\n",
      "      |=|      \n",
      "Randomized Reflex Agent had an average performance of 19807.22 over 100 trials\n",
      "Stateful Reflex Agent had an average performance of 3072.35 over 100 trials\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, envm in enumerate(envs, 1):\n",
    "    print('Environment', i)\n",
    "    envm.bump_sensor = True\n",
    "    envm.display_geography()\n",
    "    run_trials(100, envm, RandomizedReflexAgent, \"Randomized Reflex Agent\")\n",
    "    run_trials(100, envm, DepthStatefulReflexAgent, \"Stateful Reflex Agent\", sensor='bump')\n",
    "    print('------------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
