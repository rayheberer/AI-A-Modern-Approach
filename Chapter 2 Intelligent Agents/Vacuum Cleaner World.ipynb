{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9\n",
    "\n",
    "__Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.2 and specified on page 38. Your implementation should be modular so that the sensors, actuators, and environment characteristics (size, shape, dirt placement, etc.) can be changed easily. (_Note:_ for some choices of programming language and operating system there are already implementations in the [online code repository](http://aima.cs.berkeley.edu/code.html).)__\n",
    "\n",
    "The world in Figure 2.2 has two squares, \"A\" and \"B\". I have implemented this as `vacuum_cleaner_world.environments.SimpleVacuumWorld`.\n",
    "\n",
    "The specifications from page 38 are as follow:\n",
    "\n",
    "* The performance measure awards one point for each clean square at each time step, over a  \"lifetime\" of 1000 time steps.\n",
    "* The geography of the environment is known _a priori_ but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The _Left_ and _Right_ actions move the agent left and right except when this would take the agent outside the environment, in which case the agent remains where it is.\n",
    "* The only available actions are _Left_,  _Right_, and _Suck_.\n",
    "* The agent correctly perceives its location and whether that location contains dirt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10\n",
    "__Consider a modified version of the vacuum environment in Exercise 2.9, in which the agent is penalized one point for each movement.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Can a simple reflex agent be perfectly rational for this environment? Explain.__\n",
    "\n",
    "A simple reflex agent cannot be perfectly rational. Since clearly a reflex agent that returns the action `Clean` when the dirt sensor informs it that there is dirt in its location will do better than one that moves, the action for percepts `[A, Dirt]` and `[B, Dirt]` will be `Clean`. For `[A, No Dirt]` a simple reflex agent can either return `Clean`, `Left`, or `Right`.\n",
    "\n",
    "`Clean` or `Left` will cause the agent to stay in the same place for its lifetime. The expected performance of this agent will be poor, if square B is expected to have dirt. However, a reflex agent which returns `Right` for `[A, No Dirt]` and `Left` for `[B, No Dirt]` will oscillate back and forth once both squares are cleaned. An agent that stays still after visiting both squares would perform better, but no set of condition-action rules,  and therefore no simple reflex agent, can implement such an agent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. What about a reflex agent with state? Design such an agent.__\n",
    "\n",
    "Since it takes just 1 movement to visit every square, a reflex in the environment only needs to keep track of how many moves it has made, and return `NoOp` if this is equal to 1.\n",
    "\n",
    "The stateful reflex agent is implemented as `vacuum_cleaner_world.agents.StatefulReflexAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.environment import SimpleVacuumWorld\n",
    "from vacuum_cleaner_world.agents import StatefulReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleVacuumWorld(move_penalty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n",
      "1999\n",
      "1997\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "1999\n",
      "1997\n",
      "1999\n",
      "1999\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(env.simulate(StatefulReflexAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum scores attainable are:\n",
    "\n",
    "__1999__ if there is initially no dirt, or dirt in the agent's starting square...\n",
    "\n",
    "__1998__ if there is dirt in the opposite square...\n",
    "\n",
    "__1997__ if there is initially dirt in both squares...\n",
    "\n",
    "Assuming that the first score taken is after the agent makes its first move.\n",
    "\n",
    "By manually setting the initial dirt and agent location, I show that the `StatefulReflexAgent` is rational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n"
     ]
    }
   ],
   "source": [
    "dirty = SimpleVacuumWorld(move_penalty=True, dirt_init='dirty')\n",
    "print(dirty.simulate(StatefulReflexAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n"
     ]
    }
   ],
   "source": [
    "opposite1 = SimpleVacuumWorld(move_penalty=True, dirt_init=[0, 1], init_loc='A')\n",
    "print(opposite1.simulate(StatefulReflexAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n"
     ]
    }
   ],
   "source": [
    "opposite2 = SimpleVacuumWorld(move_penalty=True, dirt_init=[1, 0], init_loc='B')\n",
    "print(opposite2.simulate(StatefulReflexAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "same1 = SimpleVacuumWorld(move_penalty=True, dirt_init=[0, 1], init_loc='B')\n",
    "print(same1.simulate(StatefulReflexAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "same2 = SimpleVacuumWorld(move_penalty=True, dirt_init=[1, 0], init_loc='A')\n",
    "print(same2.simulate(StatefulReflexAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "clean = SimpleVacuumWorld(move_penalty=True, dirt_init='clean')\n",
    "print(clean.simulate(StatefulReflexAgent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
