{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9\n",
    "\n",
    "__Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.2 and specified on page 38. Your implementation should be modular so that the sensors, actuators, and environment characteristics (size, shape, dirt placement, etc.) can be changed easily. (_Note:_ for some choices of programming language and operating system there are already implementations in the [online code repository](http://aima.cs.berkeley.edu/code.html).)__\n",
    "\n",
    "The world in Figure 2.2 has two squares, \"A\" and \"B\". I have implemented this as `vacuum_cleaner_world.environments.SimpleVacuumWorld`.\n",
    "\n",
    "The specifications from page 38 are as follow:\n",
    "\n",
    "* The performance measure awards one point for each clean square at each time step, over a  \"lifetime\" of 1000 time steps.\n",
    "* The geography of the environment is known _a priori_ but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The _Left_ and _Right_ actions move the agent left and right except when this would take the agent outside the environment, in which case the agent remains where it is.\n",
    "* The only available actions are _Left_,  _Right_, and _Suck_.\n",
    "* The agent correctly perceives its location and whether that location contains dirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.environment import SimpleVacuumWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10\n",
    "__Consider a modified version of the vacuum environment in Exercise 2.9, in which the agent is penalized one point for each movement.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Can a simple reflex agent be perfectly rational for this environment? Explain.__\n",
    "\n",
    "A simple reflex agent cannot be perfectly rational. Since clearly a reflex agent that returns the action `Clean` when the dirt sensor informs it that there is dirt in its location will do better than one that moves, the action for percepts `[A, Dirt]` and `[B, Dirt]` will be `Clean`. For `[A, No Dirt]` a simple reflex agent can either return `Clean`, `Left`, or `Right`.\n",
    "\n",
    "`Clean` or `Left` will cause the agent to stay in the same place for its lifetime. The expected performance of this agent will be poor, if square B is expected to have dirt. However, a reflex agent which returns `Right` for `[A, No Dirt]` and `Left` for `[B, No Dirt]` will oscillate back and forth once both squares are cleaned. An agent that stays still after visiting both squares would perform better, but no set of condition-action rules,  and therefore no simple reflex agent, can implement such an agent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import SimpleReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleReflexAgent had an average performance of 1000 over 100 trials.\n"
     ]
    }
   ],
   "source": [
    "env = SimpleVacuumWorld(move_penalty=True)\n",
    "\n",
    "trials = 100\n",
    "scores = [env.simulate(SimpleReflexAgent) for _ in range(trials)]\n",
    "\n",
    "print('SimpleReflexAgent had an average performance of {:.0f} over {} trials.'.format(\n",
    "    sum(scores)/len(scores), \n",
    "    trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be sufficient to show that other agents can do better than this to prove that `SimpleReflexAgent` is not rational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. What about a reflex agent with state? Design such an agent.__\n",
    "\n",
    "Since it takes just 1 movement to visit every square, a reflex agent in this environment only needs to keep track of how many moves it has made, and return `NoOp` if this is equal to 1.\n",
    "\n",
    "The stateful reflex agent is implemented as `vacuum_cleaner_world.agents.StatefulReflexAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum scores attainable are:\n",
    "\n",
    "__1999__ if there is initially no dirt, or dirt in the agent's starting square...\n",
    "\n",
    "__1998__ if there is dirt in the opposite square...\n",
    "\n",
    "__1997__ if there is initially dirt in both squares...\n",
    "\n",
    "Assuming that the first score taken is after the agent makes its first move.\n",
    "\n",
    "By manually setting the initial dirt and agent location, I show that the `StatefulReflexAgent` is rational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import StatefulReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_all_dirt_agent_configs(AgentObject, **kwargs):\n",
    "    dirt_inits = ['dirty', [0, 1], [1, 0], [0, 1], [1, 0], 'clean']\n",
    "    agent_inits = [None, 'A', 'B', 'B', 'A', None]\n",
    "    simulation_names = ['Both Squares Dirty', \n",
    "                        'Agent in A, Dirt in B',\n",
    "                        'Agent in B, Dirt in A',\n",
    "                        'Agent in B, Dirt in B',\n",
    "                        'Agent in A, Dirt in A',\n",
    "                        'Both Squares Clean']\n",
    "    \n",
    "    scores = []\n",
    "    for dirt_init, agent_init, name in zip(dirt_inits, agent_inits, simulation_names):\n",
    "        env = SimpleVacuumWorld(dirt_init=dirt_init, init_loc=agent_init, **kwargs)\n",
    "        score = env.simulate(AgentObject)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return simulation_names, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Squares Dirty: 1997\n",
      "Agent in A, Dirt in B: 1998\n",
      "Agent in B, Dirt in A: 1998\n",
      "Agent in B, Dirt in B: 1999\n",
      "Agent in A, Dirt in A: 1999\n",
      "Both Squares Clean: 1999\n"
     ]
    }
   ],
   "source": [
    "names, scores = simulate_all_dirt_agent_configs(StatefulReflexAgent, move_penalty=True)\n",
    "for name, score in zip(names, scores):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c. How do your answers to *a* and *b* change if the agent's percepts give it the clean/dirty status of every square in the environment?__\n",
    "\n",
    "A simple reflex agent can be perfectly rational in this environment. Therefore, a reflex agent with state can also be perfectly rational, since it can have the same set of condition-action rules as the simple reflex agent, and maintain a state variable that does not do anything (e.g. always being equal to 0).\n",
    "\n",
    "I implement a simple reflex agent with access to all percepts as `vacuum_cleaner_world.agents.FullInfoReflexAgent`. Its condition action rules are:\n",
    "\n",
    "| Percepts                    | Action | \n",
    "| --------------------------- |:------:|\n",
    "| (`A`, `Dirt A`, `Dirt B`)   | `Suck` | \n",
    "| (`B`, `Dirt A`, `Dirt B`)   | `Suck` | \n",
    "| (`B`, `Clean A`, `Dirt B`)  | `Suck` | \n",
    "| (`A`, `Dirt A`, `Clean B`)  | `Suck` | \n",
    "| (`A`, `Clean A`, `Dirt B`)  | `Right`| \n",
    "| (`B`, `Dirt A`, `Clean B`)  | `Left` | \n",
    "| (`A`, `Clean A`, `Clean B`) | `NoOp` | \n",
    "| (`B`, `Clean A`, `Clean B`) | `NoOp` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.agents import FullInfoReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Squares Dirty: 1997\n",
      "Agent in A, Dirt in B: 1998\n",
      "Agent in B, Dirt in A: 1998\n",
      "Agent in B, Dirt in B: 2000\n",
      "Agent in A, Dirt in A: 2000\n",
      "Both Squares Clean: 2000\n"
     ]
    }
   ],
   "source": [
    "names, scores = simulate_all_dirt_agent_configs(FullInfoReflexAgent, \n",
    "                                                move_penalty=True, \n",
    "                                                perfect_information=True)\n",
    "\n",
    "for name, score in zip(names, scores):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this agent receives more percepts, its standard of rationality is higher. It can achieve a perfect score of 2000 since it can know with certainty that there is no need to visit the other if there is no dirt there initially. Note that the reflex agent with state from __b__ is still rational, since given its percepts, its expected performance measure is low if it does not visit each square once, since there is some probability of it finding dirt in the square it did not start in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 \n",
    "\n",
    "**Consider a modified version of the vacuum environment in Exercise 2.9, in which the geography of the environment - its extent, boundaries, and obstacles - is unknown, as is the initial dirt configuration. (The agent can go _Up_ and _Down_ as well as _Left_ and _Right_.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a. Can a simple reflex agent be perfectly rational for this environment? Explain.__\n",
    "\n",
    "A simple reflex agent cannot be rational, because it cannot maintain a model of the environment, which would be necessary for it to construct a model of the world. Even if it were to perceive that it was it square `A`, this information would not mean anything, because the agent would not know what squares, if any, would be next to square `A`. Since the geography of the environment is unknown, the only percept information a reflex agent can act on is that of whether there is dirt or no dirt in its current square. Therefore, the only implementable agent functions are those where the agent cleans, and then moves in a single direction whenever there is no dirt in its current square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b. Can a simple reflex agent with a _randomized_ agent function outperform a simple reflex agent? Design such an agent and measure its performance on several environments.__\n",
    "\n",
    "An agent that moves randomly when its current square is clean will outperform an agent that moves in a single direction in most environments, excepting those with the geography of a straight line where the simple reflex agent happens to start at one end and has condition action rules that cause it to move towards the other end after cleaning. Outside of these special cases, random movements will explore the environment more effectively, if not efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacuum_cleaner_world.environment import UnknownVacuumWorld\n",
    "from vacuum_cleaner_world.agents import RandomizedReflexAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedReflexAgent had an average performance of 1998.34 over 100 trials.\n"
     ]
    }
   ],
   "source": [
    "env = UnknownVacuumWorld()\n",
    "\n",
    "trials = 100\n",
    "scores = [env.simulate(RandomizedReflexAgent) for _ in range(trials)]\n",
    "\n",
    "print('RandomizedReflexAgent had an average performance of {:.2f} over {} trials.'.format(\n",
    "    sum(scores)/len(scores), \n",
    "    trials))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
